{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Users to Input their own file paths to their preprocessed data\n",
    "\n",
    "# Path to the .csv file\n",
    "FILE_PATH = \n",
    "#Example: \"/Users/jacobellen/dropbox/Preprocessed_SHARE.csv\"\n",
    "\n",
    "# Preferred location/folder to save results\n",
    "OUTPUT_FOLDER_NAME = \n",
    "#Example: \"/Users/jacobellen/dropbox/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ead4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the data, imputing and formatting the outcome frailty variable\n",
    "\n",
    "# Read in the dataset\n",
    "data = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# Initial Data Information\n",
    "data['Frailty_Score'].describe()\n",
    "\n",
    "# Impute missing data by medican imputation\n",
    "median_imp = list(data.loc[:, \"pmh_heart_attack\" : \"drugs_for_other\"].columns)\n",
    "median_imp.extend([\"ever_smoked_daily\", \"depression\", \"outpatient_surgery\",\n",
    "                   \"vitals_measured_weight\", \"vitals_height\", \"vitals_BMI\", \"day_per_week_alcohol\", \n",
    "                   \"hospital_stays_lastyear\"\n",
    "                  ])\n",
    "for col in median_imp:\n",
    "    data[col].fillna(data[col].median(), inplace=True)\n",
    "       \n",
    "# Target Variable\n",
    "data[\"Frailty_Score\"] = data[\"Frailty_Score\"].apply(lambda x: 1 if x == \"Frail\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries for storing results\n",
    "\n",
    "# Binary Classification Results\n",
    "results = {\n",
    "    \"All\": {\"Accuracy\": [], \"AUC\": [], \"Sensitivity\": [], \"Specificity\": [], \"PPV\": [], \"NPV\": []},\n",
    "    \"Normal\": {\"Accuracy\": [], \"AUC\": [], \"Sensitivity\": [], \"Specificity\": [], \"PPV\": [], \"NPV\": []},\n",
    "    \"Prefrail\": {\"Accuracy\": [], \"AUC\": [], \"Sensitivity\": [], \"Specificity\": [], \"PPV\": [], \"NPV\": []},\n",
    "    \"12_26_Months\": {\"Accuracy\": [], \"AUC\": [], \"Sensitivity\": [], \"Specificity\": [], \"PPV\": [], \"NPV\": []},\n",
    "    \"27_33_Months\": {\"Accuracy\": [], \"AUC\": [], \"Sensitivity\": [], \"Specificity\": [], \"PPV\": [], \"NPV\": []},\n",
    "    \"34_48_Months\": {\"Accuracy\": [], \"AUC\": [], \"Sensitivity\": [], \"Specificity\": [], \"PPV\": [], \"NPV\": []},\n",
    "}\n",
    "\n",
    "# Triclass Classification Results\n",
    "metrics_history = {\n",
    "    'Accuracy': [],\n",
    "    'F1': [],\n",
    "    'AUC': [],\n",
    "    'Sensitivity': [],\n",
    "    'Specificity': [],\n",
    "    'PPV': [],\n",
    "    'NPV': []\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store all FPR, TPR, and AUC for different subsets across all runs\n",
    "roc_data = {\n",
    "    'All_Data': {'fpr': [], 'tpr': [], 'auc': []},\n",
    "    '12-26 Months': {'fpr': [], 'tpr': [], 'auc': []},\n",
    "    '27-33 Months': {'fpr': [], 'tpr': [], 'auc': []},\n",
    "    '34-48 Months': {'fpr': [], 'tpr': [], 'auc': []},\n",
    "}\n",
    "\n",
    "# Variable importance lists\n",
    "PredChange = []\n",
    "AUCChange_List = [] \n",
    "\n",
    "# Random states\n",
    "random_states = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random seed and train/test splits for reproducibility\n",
    "\n",
    "SEED = 30\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Define StratifiedKFold for consistent splits\n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Create Train-Test Splits for Binary Classification\n",
    "binary_splits = list(skf.split(data.drop(columns=[\"Frailty_Score\"]), data[\"Frailty_Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03204934",
   "metadata": {},
   "source": [
    "### Binary Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4114cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at all the columns in the dataframe\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b95c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at how the dataset is structured\n",
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a757b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking at distribution of the outcome frailty variable \n",
    "data.groupby([\"Frailty_Score\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2bade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions needed to perform binary classification task\n",
    "\n",
    "# Summarizing results\n",
    "def summarize_and_save_results(results, folder_path):\n",
    "    \"\"\"\n",
    "    Summarize and save model performance metrics for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "    results (dict): Dictionary containing performance metrics for different subsets.\n",
    "    folder_path (str): Path to the folder where results should be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    summary = {}\n",
    "    for subset, metrics in results.items():\n",
    "        summary[subset] = {\n",
    "            'Accuracy': f\"{np.mean(metrics['Accuracy']):.4f} ± {np.std(metrics['Accuracy']):.4f}\",\n",
    "            'AUC': f\"{np.mean(metrics['AUC']):.4f} ± {np.std(metrics['AUC']):.4f}\",\n",
    "            'Sensitivity': f\"{np.mean(metrics['Sensitivity']):.4f} ± {np.std(metrics['Sensitivity']):.4f}\",\n",
    "            'Specificity': f\"{np.mean(metrics['Specificity']):.4f} ± {np.std(metrics['Specificity']):.4f}\",\n",
    "            'PPV': f\"{np.mean(metrics['PPV']):.4f} ± {np.std(metrics['PPV']):.4f}\",\n",
    "            'NPV': f\"{np.mean(metrics['NPV']):.4f} ± {np.std(metrics['NPV']):.4f}\"\n",
    "        }\n",
    "\n",
    "    # Save results to a DataFrame\n",
    "    summary_df = pd.DataFrame.from_dict(summary, orient='index')\n",
    "    output_file = os.path.join(folder_path, \"Summary_Metrics.csv\")\n",
    "    summary_df.to_csv(output_file)\n",
    "    print(summary_df)\n",
    "    print(f\"Summary metrics saved to: {output_file}\")\n",
    "\n",
    "# ROC Curve Analysis\n",
    "def roc_analysis(y_test, y_prob):\n",
    "    \"\"\"\n",
    "    Perform ROC analysis and plot the ROC curve for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "    y_test (numpy.ndarray): True labels for the test set.\n",
    "    y_prob (numpy.ndarray): Predicted probabilities for the test set.\n",
    "\n",
    "    Returns:\n",
    "    float: Optimal threshold for maximizing sensitivity and specificity.\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Optimal Threshold (Maximizes Sensitivity + Specificity)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', label=f'Optimal Threshold = {optimal_threshold:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Optimal Threshold for Sensitivity & Specificity: {optimal_threshold:.2f}')\n",
    "    return optimal_threshold\n",
    "\n",
    "# Define function to store ROC data for each subset\n",
    "def store_roc_data(y_true, y_prob, key):\n",
    "    \"\"\"\n",
    "    Store ROC curve data (FPR, TPR, and AUC) for a specific subset of data.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy.ndarray): True labels.\n",
    "    y_prob (numpy.ndarray): Predicted probabilities.\n",
    "    key (str): Subset identifier for storing data.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Ensure the key exists in the roc_data dictionary\n",
    "    if key not in roc_data:\n",
    "        roc_data[key] = {'fpr': [], 'tpr': [], 'auc': []}\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_data[key]['fpr'].append(fpr)\n",
    "    roc_data[key]['tpr'].append(tpr)\n",
    "    roc_data[key]['auc'].append(roc_auc)\n",
    "\n",
    "# Helper function to calculate metrics and store them\n",
    "def process_subset(model, X, y):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on a test subset and calculate performance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    model (keras.Model): Trained Keras model.\n",
    "    X (numpy.ndarray): Features for the test set.\n",
    "    y (numpy.ndarray): True labels for the test set.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Metrics including accuracy, AUC, sensitivity, specificity, PPV, and NPV.\n",
    "    \"\"\"\n",
    "    y_prob = model.predict(X).flatten()\n",
    "    optimal_idx = np.argmax(roc_curve(y, y_prob)[1] - roc_curve(y, y_prob)[0])\n",
    "    optimal_threshold = roc_curve(y, y_prob)[2][optimal_idx]\n",
    "    y_pred = (y_prob >= optimal_threshold).astype(int)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    accuracy, sensitivity, specificity, ppv, npv = calculate_conf_metrics(cm)\n",
    "    auc_score = roc_auc_score(y, y_prob)\n",
    "    return accuracy, auc_score, sensitivity, specificity, ppv, npv\n",
    "\n",
    "def calculate_conf_metrics(cm):\n",
    "    \"\"\"\n",
    "    Calculate metrics (accuracy, sensitivity, specificity, PPV, and NPV) from a confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "    cm (numpy.ndarray): Confusion matrix (2x2 array).\n",
    "\n",
    "    Returns:\n",
    "    tuple: Metrics including accuracy, sensitivity, specificity, PPV, and NPV.\n",
    "    \"\"\"\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) != 0 else 0\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    ppv = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    npv = TN / (TN + FN) if (TN + FN) != 0 else 0\n",
    "    return accuracy, sensitivity, specificity, ppv, npv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c93f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for Modeling for Binary Classification - 5 train/test splits that are averaged\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(binary_splits):\n",
    "    print(f\"Binary Classification - Fold {fold+1}\")\n",
    "    \n",
    "    # Split Data Using Predefined Indices\n",
    "    train = data.iloc[train_idx]\n",
    "    test = data.iloc[test_idx]\n",
    "    \n",
    "    # SMOTE Oversampling\n",
    "    print(\"Train Class Distribution Before SMOTE:\", Counter(train[\"Frailty_Score\"]))\n",
    "    X = train.drop(columns=[\"Frailty_Score\", \"Baseline_Frailty\"])\n",
    "    y = train[\"Frailty_Score\"]\n",
    "    smote = SMOTE(sampling_strategy=0.90, random_state=SEED)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    print(\"Train Class Distribution After SMOTE:\", Counter(y_resampled))\n",
    "    \n",
    "    # Combine Resampled Data\n",
    "    train = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), \n",
    "                       pd.DataFrame(y_resampled, columns=[\"Frailty_Score\"])], axis=1)\n",
    "    features = list(train.loc[:, train.columns != \"Frailty_Score\"].columns)\n",
    "    output = \"Frailty_Score\"\n",
    "    \n",
    "    # Finalizing the Training Set\n",
    "    train = pd.DataFrame(train)\n",
    "    \n",
    "    # Splitting Testing Set by Baseline Frailty\n",
    "    test_normal = test[test['Baseline_Frailty'] == 0]\n",
    "    test_prefrail = test[test['Baseline_Frailty'] == 1]\n",
    "    \n",
    "    # Dropping Baseline Frailty so it is not used as a predictor\n",
    "    test_normal = test_normal.drop(columns=['Baseline_Frailty'], axis=1)\n",
    "    test_prefrail = test_prefrail.drop(columns=['Baseline_Frailty'], axis=1)\n",
    "    test = test.drop(columns=['Baseline_Frailty'], axis=1)\n",
    "\n",
    "    # Finalizing Testing Sets\n",
    "    test = pd.DataFrame(test)\n",
    "    test_prefrail = pd.DataFrame(test_prefrail)\n",
    "    test_normal = pd.DataFrame(test_normal)\n",
    "\n",
    "    # Splitting Testing Set by Months Between Appointments\n",
    "    test_12_26 = test[(test['Months_Between_Appointments'] >= 12) & (test['Months_Between_Appointments'] <= 26)]\n",
    "    test_27_33 = test[(test['Months_Between_Appointments'] > 26) & (test['Months_Between_Appointments'] < 34)]\n",
    "    test_34_48 = test[(test['Months_Between_Appointments'] >= 34) & (test['Months_Between_Appointments'] <= 48)]\n",
    "    \n",
    "    # Getting Size of Each Division by Time\n",
    "    print(\"12-26 Group Size:\", len(test_12_26))\n",
    "    print(\"27-33 Group Size:\", len(test_27_33))\n",
    "    print(\"34-48 Group Size:\", len(test_34_48))\n",
    "    \n",
    "    # Split train/test into features and labels\n",
    "    X_train = train.drop('Frailty_Score', axis=1) \n",
    "    y_train = train.Frailty_Score  \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=SEED) \n",
    "\n",
    "    # Finalizing all the Test Subsets (\"TestN\" = Normal Patients; \"TestF\" = Prefrail Patients)\n",
    "    testN = test_normal\n",
    "    testF = test_prefrail\n",
    "    X_test = test.drop('Frailty_Score', axis=1) \n",
    "    y_test = test.Frailty_Score\n",
    "\n",
    "    # Initiating the Model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=(X_train.shape[1]), activation='relu'))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    \n",
    "    # Compiling the Model\n",
    "    model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\n",
    "        keras.metrics.SensitivityAtSpecificity(0.5, num_thresholds=10),\n",
    "        keras.metrics.SpecificityAtSensitivity(0.5, num_thresholds=10),\n",
    "        keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    # Training the Model\n",
    "    history = model.fit(\n",
    "    X_train, y_train, validation_data=(X_val, y_val),\n",
    "    epochs=25, batch_size=64, verbose=1)\n",
    "\n",
    "    # Getting Values for Feature Importance Analysis\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    X_test = X_test.values\n",
    "\n",
    "    # Optimal sensitivity/specifity for all patients using ROC\n",
    "    y_prob = model.predict(X_test).flatten()\n",
    "    optimal_threshold_all = roc_analysis(y_test, y_prob)\n",
    "    y_pred = (y_prob >= optimal_threshold_all).astype(int)\n",
    "    \n",
    "    # Calculate importance scores for each feature \n",
    "    original_predictions = model.predict(X_test)\n",
    "    y_pred_baseline = (original_predictions >= optimal_threshold_all).astype(int)\n",
    "    importance_change = []\n",
    "    auc_importance = []\n",
    "\n",
    "    for feature_index in range(X_test.shape[1]):\n",
    "        perturbed_data = X_test.copy()\n",
    "        perturbed_data[:, feature_index] = np.random.permutation(perturbed_data[:, feature_index])\n",
    "        perturbed_predictions = model.predict(perturbed_data)\n",
    "        \n",
    "        # Change in predictions\n",
    "        importance_predchange = np.mean(np.abs(perturbed_predictions - original_predictions))\n",
    "        importance_change.append(importance_predchange)\n",
    "               \n",
    "        # Calculate the change in AUC when the feature is perturbed\n",
    "        baseline_auc = roc_auc_score(y_test, y_prob) \n",
    "        perturbed_prob = model.predict(perturbed_data).flatten()\n",
    "        perturbed_auc = roc_auc_score(y_test, perturbed_prob)\n",
    "        auc_drop = baseline_auc - perturbed_auc\n",
    "        auc_importance.append(auc_drop)\n",
    "\n",
    "    # Create a DataFrame containing the feature names and their importance scores\n",
    "    importance_change = np.array(importance_change)\n",
    "    Most_Important_Change = pd.DataFrame({\n",
    "        'Variable': feature_names,\n",
    "        'Importance': importance_change })\n",
    "    importance_auc = np.array(auc_importance) \n",
    "    Most_Important_AUC = pd.DataFrame({\n",
    "        'Variable': feature_names,\n",
    "        'Importance': importance_auc })\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(Most_Important_Change)\n",
    "\n",
    "    # Evaluation and Metrics\n",
    "    metrics_lists = {\n",
    "        'sensitivity': [], 'specificity': [], 'accuracy': [], 'f1': [],\n",
    "        'precision': [], 'misclassification': [], 'npv': []\n",
    "    }\n",
    "    threshold_list = []\n",
    "\n",
    "    # Evaluate on test sets\n",
    "    for subset_name, subset_data in [\n",
    "        (\"All\", test),\n",
    "        (\"Normal\", testN),\n",
    "        (\"Prefrail\", testF),\n",
    "        (\"12_26_Months\", test_12_26),\n",
    "        (\"27_33_Months\", test_27_33),\n",
    "        (\"34_48_Months\", test_34_48),\n",
    "    ]:\n",
    "        X_test = subset_data.drop(columns=[\"Frailty_Score\"])\n",
    "        y_test = subset_data[\"Frailty_Score\"]\n",
    "        accuracy, auc_score, sensitivity, specificity, ppv, npv = process_subset(model, X_test, y_test)\n",
    "        results[subset_name]['Accuracy'].append(accuracy)\n",
    "        results[subset_name]['AUC'].append(auc_score)\n",
    "        results[subset_name]['Sensitivity'].append(sensitivity)\n",
    "        results[subset_name]['Specificity'].append(specificity)\n",
    "        results[subset_name]['PPV'].append(ppv)\n",
    "        results[subset_name]['NPV'].append(npv)\n",
    "        y_prob_roc = model.predict(X_test).flatten()\n",
    "        store_roc_data(y_test, y_prob_roc, subset_name)\n",
    "    \n",
    "    # Important variables\n",
    "    PredChange.append(Most_Important_Change)\n",
    "    AUCChange_List.append(Most_Important_AUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6698dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up Length AUC Figure for Binary Classification\n",
    "\n",
    "# Define the groups to include in the plot\n",
    "groups_to_plot = [\"All\", \"12_26_Months\", \"27_33_Months\", \"34_48_Months\"]\n",
    "\n",
    "# After running 5-fold loop, compute the mean TPR at each FPR grid point\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# Start plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Define a consistent, publication-friendly color palette\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# Update group labels\n",
    "label_mapping = {\n",
    "    \"All\": \"All Patients\",\n",
    "    \"12_26_Months\": \"12-26 Months\",\n",
    "    \"27_33_Months\": \"27-33 Months\",\n",
    "    \"34_48_Months\": \"34-48 Months\"\n",
    "}\n",
    "\n",
    "# Iterate over the specified groups only\n",
    "for i, group in enumerate(groups_to_plot):\n",
    "    if group not in roc_data:\n",
    "        continue\n",
    "    \n",
    "    # Get the data for the group\n",
    "    data = roc_data[group]\n",
    "    display_key = label_mapping.get(group, group)\n",
    "    \n",
    "    mean_tpr = np.zeros_like(mean_fpr)\n",
    "    \n",
    "    # Interpolate each ROC curve and add it to the mean_tpr\n",
    "    for fpr, tpr in zip(data['fpr'], data['tpr']):\n",
    "        mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "    \n",
    "    mean_tpr /= len(data['fpr'])  # Average it\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    \n",
    "    # Plot with improved formatting\n",
    "    ax.plot(\n",
    "        mean_fpr,\n",
    "        mean_tpr,\n",
    "        label=f'{display_key} (AUC = {mean_auc:.2f})',\n",
    "        lw=2,\n",
    "        color=colors[i % len(colors)]\n",
    "    )\n",
    "\n",
    "# Plot diagonal line for reference\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', color='gray', lw=1)\n",
    "\n",
    "# Improve axis titles and labels\n",
    "ax.set_xlabel('False Positive Rate (1-Specificity)', fontsize=14)\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity)', fontsize=14)\n",
    "\n",
    "# Improve the legend\n",
    "ax.legend(\n",
    "    loc='lower right',\n",
    "    fontsize=12,\n",
    "    frameon=True,\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "# Add gridlines for clarity\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Optimize spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6131830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Important Variables Code\n",
    "\n",
    "# Concatenating dataframes into a single dataframe\n",
    "combined_df_change = pd.concat(PredChange, ignore_index=True)\n",
    "\n",
    "# Grouping by 'Variable' and computing mean and standard deviation of 'Importance'\n",
    "result_df_change = combined_df_change.groupby('Variable').agg(\n",
    "    Average_Importance=('Importance', 'mean'),\n",
    "    Std_Dev_Importance=('Importance', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Sorting the resulting DataFrame by 'Average_Importance' in descending order\n",
    "Change_DF = result_df_change.sort_values(by='Average_Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(Change_DF)\n",
    "\n",
    "\n",
    "# Accuracy Most Important Variables\n",
    "\n",
    "# Concatenating dataframes into a single dataframe\n",
    "combined_df_auc = pd.concat(AUCChange_List, ignore_index=True)\n",
    "\n",
    "# Grouping by 'Variable' and computing mean and standard deviation of 'Importance'\n",
    "result_df_auc = combined_df_auc.groupby('Variable').agg(\n",
    "    Average_AUC_Decrease=('Importance', 'mean'),\n",
    "    Std_Dev_AUC=('Importance', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Sorting the resulting DataFrame by 'Average_Importance' in descending order\n",
    "AUC_DF = result_df_auc.sort_values(by='Average_AUC_Decrease', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(AUC_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1edb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize and save the results\n",
    "summarize_and_save_results(results, OUTPUT_FOLDER_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
